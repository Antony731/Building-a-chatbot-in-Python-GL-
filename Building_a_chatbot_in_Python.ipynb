{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building a chatbot in Python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJeMI0m-i8UP"
      },
      "source": [
        "Importing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXi62JRAinMy"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNLh_AAgjIGL"
      },
      "source": [
        "importing and reading the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luQ4b8y4jGFs",
        "outputId": "9c551d66-ef61-499f-e1ac-a35fa320a237"
      },
      "source": [
        "f = open('chatbot.txt','r', errors = 'ignore')\n",
        "raw_doc = f.read()\n",
        "raw_doc = raw_doc.lower()\n",
        "nltk.download('punkt') #Using Punkt tokenizer\n",
        "nltk.download('wordnet') #Using WordNet dictionary\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc) #Converts doc to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw_doc) # Converts doc to list of words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4wFQNFfozvc",
        "outputId": "b5c44029-5f59-4af7-a8ec-248f5d2e9b35"
      },
      "source": [
        "sent_tokens[:2]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n",
              " 'data science is related to data mining, machine learning and big data.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF36lmgARmQN",
        "outputId": "d36e3e5d-b502-40d8-bb6f-7679fcdbeb7b"
      },
      "source": [
        "word_tokens[:2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'science']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxhf82DIR5A-"
      },
      "source": [
        "Text Preprocesssing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "431pFoZPR2Bx"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzDQpABcXdK2"
      },
      "source": [
        "**Defining the Greeting Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXmrNNRtTbHm"
      },
      "source": [
        "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
        "GREET_RESPONSES = [\"hi\", \"hey\", \"nods\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDedOhN1Ytv5"
      },
      "source": [
        "**RESPONSE GENERATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6nsfBjcYXvn"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoBiRpbKZW_t"
      },
      "source": [
        "def response(user_response):\n",
        "  robo1_response = ''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words='english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response = robo1_response +\"I am sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response =  robo1_response+sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lLKw17TdR0z"
      },
      "source": [
        "**Defining conversation start/end protocol**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNjVRS1Db2h7",
        "outputId": "e284bee7-344d-4898-eebf-84383c13fd62"
      },
      "source": [
        "flag = True\n",
        "print(\"BOT: My name is Stark. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response!= 'bye'):\n",
        "    if(user_response=='thanks' or user_response == 'thank you'):\n",
        "      flag = False\n",
        "      print(\"BOT: You are welcome...\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT: \"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens = word_tokens+nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print(\"BOT: \", end = \"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"BOT: Goodbye! Take care <3\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOT: My name is Stark. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n",
            "Hey\n",
            "BOT: I am glad! You are talking to me\n",
            "Hello\n",
            "BOT: hey\n",
            "Hello\n",
            "BOT: hi there\n",
            "Content\n",
            "BOT: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[4][5]\n",
            "contents\n",
            "\n",
            "    1 foundations\n",
            "        1.1 relationship to statistics\n",
            "    2 etymology\n",
            "        2.1 early usage\n",
            "        2.2 modern usage\n",
            "    3 impact\n",
            "    4 technologies and techniques\n",
            "        4.1 techniques\n",
            "    5 references\n",
            "\n",
            "foundations\n",
            "\n",
            "data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n",
            "foundations\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BOT: [4][5]\n",
            "contents\n",
            "\n",
            "    1 foundations\n",
            "        1.1 relationship to statistics\n",
            "    2 etymology\n",
            "        2.1 early usage\n",
            "        2.2 modern usage\n",
            "    3 impact\n",
            "    4 technologies and techniques\n",
            "        4.1 techniques\n",
            "    5 references\n",
            "\n",
            "foundations\n",
            "\n",
            "data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n",
            "nujsrgbure\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BOT: I am sorry! I don't understand you\n",
            "data science\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BOT: \"data science\".\n",
            "reference\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BOT: machine learning is a technique used to perform tasks by inferencing patterns from data\n",
            "\n",
            "references\n",
            "\n",
            "dhar, v. (2013).\n",
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BOT: retrieved 3 april 2020.\n",
            "bye\n",
            "BOT: Goodbye! Take care <3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f04Ms-etiQjN"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}